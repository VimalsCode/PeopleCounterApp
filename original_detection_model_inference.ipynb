{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code performs inference with the original object detection model. The output frame with bounding box will be saved separately from the frame without person or false positive where model failed to detect.<br>\n",
    "model_type variable has to be updated with corrected model - ssd or rcnn. <br>\n",
    "model_name variable should contain the model to be used for inference.<br>\n",
    "The output folder to be created will be rcnn_output_detected_images and rcnn_output_images respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import cv2 as cv2\n",
    "import pathlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing video from file\n",
    "cap = cv2.VideoCapture('Pedestrian_Detect_2_1_1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "    model_file = model_name + '.tar.gz'\n",
    "    model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name, \n",
    "    origin=base_url + model_file,\n",
    "    untar=True)\n",
    "\n",
    "    model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "    model = tf.saved_model.load(str(model_dir))\n",
    "    model = model.signatures['serving_default']\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# ssd or rcnn\n",
    "model_type = \"rcnn\"\n",
    "#model_name = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
    "model_name = 'faster_rcnn_inception_v2_coco_2018_01_28'\n",
    "HEIGHT = 600\n",
    "WIDTH = 1024\n",
    "#load the model\n",
    "detection_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'image_tensor:0' shape=(None, None, None, 3) dtype=uint8>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'detection_scores': tf.float32,\n",
       " 'detection_classes': tf.float32,\n",
       " 'num_detections': tf.float32,\n",
       " 'detection_boxes': tf.float32}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frame counter\n",
    "count = 0\n",
    "print(detection_model.inputs)\n",
    "detection_model.output_dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(model, frame):\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "    \n",
    "    # Run inference\n",
    "    output_dict = model(input_tensor)\n",
    "    \n",
    "    #print(output_dict)\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "        \n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started video inference....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vimal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[380.33328]\n",
      "Error\n",
      "[87  1  1]\n",
      "Error\n",
      "[ 1 87  1]\n",
      "Error\n",
      "[ 1 87  1 28]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-74a711839b12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m                         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m55\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                         \u001b[1;31m# store the images with bounding box\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_output_detected_images\\image\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;31m# store the images - probability is less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Grab the shape of the input \n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "print(\"Started video inference....\")\n",
    "total_inf_start = time.time()\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    image = cv2.resize(frame, (HEIGHT, WIDTH))\n",
    "    # Actual inference.    \n",
    "    # Start asynchronous inference for specified request.\n",
    "    frame_inf_start = time.time()\n",
    "    output_dict = perform_inference(detection_model, image)\n",
    "    frame_det_time = time.time() - frame_inf_start\n",
    "    inf_time_message = \"Inference time: {:.3f}ms\"\\\n",
    "                               .format(frame_det_time * 1000)\n",
    "    cv2.putText(frame, inf_time_message, (15, 15),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 0.5, (200, 10, 10), 1)\n",
    "    if 1 in output_dict['detection_classes']:\n",
    "        if len(output_dict['detection_classes']) > 1:\n",
    "            a, cnts = np.unique(output_dict['detection_classes'], return_counts=True)\n",
    "            if a[cnts > 1] and a[cnts.argmax()] == 1:\n",
    "                print(\"Error\")\n",
    "                print(output_dict['detection_classes'])\n",
    "                # store the images\n",
    "                cv2.imwrite(model_type+\"_output_images\\image\"+str(count)+\".jpg\", frame)\n",
    "            else:\n",
    "                # get the index for person class\n",
    "                index = np.where(output_dict['detection_classes'] == 1)\n",
    "                # filter based on probability score\n",
    "                if output_dict['detection_scores'][index] >= 0.4:\n",
    "                    values = output_dict['detection_boxes'][index]\n",
    "                    for value in values:\n",
    "                        xmin = int(value[1] * width)\n",
    "                        ymin = int(value[0] * height)\n",
    "                        xmax = int(value[3] * width)\n",
    "                        ymax = int(value[2] * height)\n",
    "                        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 55, 255), 1)\n",
    "                        # store the images with bounding box\n",
    "                        cv2.imwrite(model_type+\"_output_detected_images\\image\"+str(count)+\".jpg\", frame)\n",
    "                else:\n",
    "                    # store the images - probability is less\n",
    "                    cv2.imwrite(model_type+\"_output_images\\image\"+str(count)+\".jpg\", frame)\n",
    "        else:\n",
    "            if output_dict['detection_scores'] >= 0.4 and output_dict['detection_classes'] == 1:\n",
    "                xmin = int(output_dict['detection_boxes'][0][1] * width)\n",
    "                ymin = int(output_dict['detection_boxes'][0][0] * height)\n",
    "                xmax = int(output_dict['detection_boxes'][0][3] * width)\n",
    "                ymax = int(output_dict['detection_boxes'][0][2] * height)\n",
    "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 55, 255), 1)\n",
    "                # store the images with bounding box\n",
    "                cv2.imwrite(model_type+\"_output_detected_images\\image\"+str(count)+\".jpg\", frame)\n",
    "            else:\n",
    "                # store the images - probability is less\n",
    "                print(count + output_dict['detection_scores'])\n",
    "                cv2.imwrite(model_type+\"_output_images\\image\"+str(count)+\".jpg\", frame)\n",
    "    else:\n",
    "        # store the images\n",
    "         cv2.imwrite(model_type+\"_output_images\\image\"+str(count)+\".jpg\", frame)\n",
    "                \n",
    "    \n",
    "    count = count + 1    \n",
    "\n",
    "# When everything done, release the video capture and video write objects\n",
    "cap.release()\n",
    "\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()    \n",
    "print(\"Total Frame processed:\",count)\n",
    "total_det_time = time.time() - total_inf_start\n",
    "total_inf_time_message = \"Inference time: {:.3f}ms\"\\\n",
    "                               .format(total_det_time * 1000)\n",
    "print(total_inf_time_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
